import torch
import torch.nn as nn
import numpy as np
from .. import BaseModel_backbone_with_neck

# Define functions.

# 1x1 convolution, padding = 0.
def con1x1_0(inpCha, outCha, kernel_size=1, stride=1, bias=True):  # padding 0 by default.
    return nn.Conv2d(inpCha, outCha, kernel_size=kernel_size, stride=stride, bias=bias)


# 3x3 convolution, padding = 0.
def con3x3_0(inpCha, outCha, kernel_size=3, stride=1, bias=True):
    return nn.Conv2d(inpCha, outCha, kernel_size=kernel_size, stride=stride, bias=bias)


# 3x3 convolution with padding = 1.
def con3x3_1(inpCha, outCha, kernel_size=3, stride=1, padding=1, bias=True):
    return nn.Conv2d(inpCha, outCha, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)


# 2x2 transposed convolution.
def traCon2x2(inpCha, outCha, kernel_size, stride, padding, output_padding, bias=True):
    return nn.ConvTranspose2d(inpCha, outCha, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding, bias=bias)


# ReLU Activation function.
def actReLU():
    return nn.ReLU(inplace=True)

class EncBlock(nn.Module):
    """
    Build one block of the encoding branch.
    Parameters
    ----------
    inpCha : integer
        Input channels for the convolution.
    outCha : integer
        Output channels of the convolution (number of kernels).
    pooling : bool
        Whether to perform the pooling or not at the end of the block.
    poolPad : tuple of two integers
        Padding to be used in the pooling.
    Methods
    -------
    forward : forward pass into the block
    """
    def __init__(self, inpCha, outCha, pooling, poolPad):
        super(EncBlock, self).__init__()

        self.pooling = pooling

        self.con0 = con3x3_1(inpCha, outCha)
        self.con1 = con3x3_1(outCha, outCha)

        self.act = actReLU()

        if self.pooling:
            self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=poolPad)

    def forward(self, x):
        """
        Perform the forward pass through the block.
        Parameters
        ----------
        x : torch tensor
            4D tensor, output of the block.
                first dimension = batch size
                second dimension = number of channels
                third dimension = number of columns
                fourth dimension = number of rows
        Returns
        -------
        x : torch tensor
            4D tensor output by the block.
              first dimension = batch size
              second dimension = number of channels
              third dimension = number of columns
              fourth dimension = number of rows
        x_0 : torch tensor
            4D tensor output by the block before pooling. Dimensions as above.
        """
        x = self.act(self.con0(x))
        x = self.act(self.con1(x))
        xOut = x
        if self.pooling:
            x = self.pool(x)

        # print("\n EncBlock Output Shape = ", x.shape)

        return x, xOut

class DecBlock(nn.Module):
    """
    Block of the decoding path.
    Parameters
    ----------
    inpCha : integer
        Input channels for the convolution.
    outCha : integer
        Output channels of the convolution (number of kernels).
    tranPad : tuple of two intergers
        Padding to be used in the transposed convolution.
    finConv : bool
        Define whether to do the last 1x1 convolution or not.
    Methods
    -------
    forward : forward pass into the block
    """
    def __init__(self, inpCha, outCha, tranPad, finConv=False, finOutCha=1):
        super(DecBlock, self).__init__()

        self.traCon = traCon2x2(inpCha, outCha, kernel_size=2, stride=2,
                                padding=tranPad, output_padding=(0, 0))
        self.con0 = con3x3_1(2*outCha, outCha)
        self.con1 = con3x3_1(outCha, outCha)

        self.act = actReLU()

        self.finConv = finConv
        if self.finConv:
            self.con2 = con1x1_0(outCha, finOutCha)

    def forward(self, xDown, x):
        """
        Perform the forward pass through the block.
        Parameters
        ----------
        xDown : torch tensor
            4D tensor generated by the corresding block in the
            downsampling path.
              first dimension = batch size
              second dimension = number of channels
              third dimension = number of columns
              fourth dimension = number of rows
        x : torch tensor
            4D tensor, output of the block.
              first dimension = batch size
              second dimension = number of channels
              third dimension = number of columns
              fourth dimension = number of rows
        Returns
        -------
        x : torch tensor
            4D tensor output by the block.
              first dimension = batch size
              second dimension = number of channels
              third dimension = number of columns
              fourth dimension = number of rows
        """
        xUp = self.traCon(x)
        x = torch.cat((xUp, xDown), 1)

        x = self.act((self.con0(x)))
        x = self.act((self.con1(x)))

        if self.finConv:
            x = self.con2(x)

        # print("\n     DecBlock Output Shape = ", x.shape)

        return x

class UNetX(BaseModel_backbone_with_neck):
    """
    The network consists of an encoding branch followed by a decoding one.
    Each block is made by blocks performing 2 convolutions each.
    The last block of the decoder also performs a 1x1 convolution.
    Parameters
    ----------
    inpCha : integer
        Number of channels in the input tensor.
    finOutCha : integer
        Number of channels of the network output.
    lstKer : list of integers
        First number is the number of input channels.
        The remaining numbers give the number of kernels used
        in the convolutions performed in the downsampling and
        upsampling paths.
        The length of this list, minus one,
        gives the number of downsampling blocks.
        The number of Upsampling blocks is len(lstKer)-2.
    inpWidth : integer
        Number of columns of the input image.
    inpHeight : integer
        Number of rows of the input image.
    Methods
    -------
    forward : forward pass into the network
    """
    def __init__(self, inpCha, finOutCha, lstKer, inpWidth, inpHeight,
                 **kwargs):
        super(UNetX, self).__init__(**kwargs)

        # If input height and/or width are odd, the input has been padded.
        # Correct the input width and heigth.
        if inpWidth % 2 != 0:
            inpWidth += 1
        if inpHeight % 2 != 0:
            inpHeight += 1

        # Adapt the list lstKer.
        lstKer.insert(0, inpCha)
        self.lstKer = lstKer

        self.down_convs = []
        self.up_convs = []

        # Build the encoder.
        # List_0 = [inpWidth]
        # List_1 = [inpHeight]
        numDowns = len(lstKer)-1  # number of downsampling blocks
        # pad0 = 0
        # pad1 = 0
        for i in range(numDowns):
            pooling = True if i < numDowns-1 else False
            # # avoid odd num. using padding.
            # if (pooling and i < numDowns - 2):
            #     if (inpWidth/2) % 2 == 0:
            #         pad0 = 0
            #         inpWidth //= 2
            #         List_0.append(inpWidth)
            #     else:
            #         pad0 = 1
            #         inpWidth = inpWidth // 2 + 1
            #         List_0.append(inpWidth)
            #     if (inpHeight/2) % 2 == 0:
            #         pad1 = 0
            #         inpHeight //= 2
            #         List_1.append(inpHeight)
            #     else:
            #         pad1 = 1
            #         inpHeight = inpHeight // 2 + 1
            #         List_1.append(inpHeight)
            # # do not avoid odd numbers at the bottom.
            # if (pooling and i == numDowns - 2):
            #     pad0 = 0
            #     inpWidth //= 2
            #     List_0.append(inpWidth)
            #     pad1 = 0
            #     inpHeight //= 2
            #     List_1.append(inpHeight)
            block = EncBlock(lstKer[i], lstKer[i+1], pooling, (0, 0))
            self.down_convs.append(block)

        # Build the decoder.
        # List_0.reverse()
        # List_1.reverse()
        numUps = len(lstKer) - 4  # number of upsampling blocks
        self.lstKer.reverse()  # reverse the list of the lstKer
        for i in range(numUps):
            # if List_0[i]*2 == List_0[i+1]:
            #     pad0 = 0
            # else:
            #     pad0 = 1
            # if List_1[i]*2 == List_1[i+1]:
            #     pad1 = 0
            # else:
            #     pad1 = 1
            if i < (numUps - 1):
                block = DecBlock(lstKer[i], lstKer[i+1], (0, 0))
                self.up_convs.append(block)
            else:
                block = DecBlock(lstKer[i], lstKer[i+1], (0, 0),
                                finConv=True, finOutCha=finOutCha)
                self.up_convs.append(block)

        self.down_convs = nn.ModuleList(self.down_convs)
        self.up_convs = nn.ModuleList(self.up_convs)

    def forward(self, x):
        """
        Perform the forward pass through the network.
        Parameters
        ----------
        x : torch tensor
            4D tensor defining input to be fed into the network.
                first dimension = batch size
                second dimension = number of channels
                third dimension = number of columns of the input image
                fourth dimension = Number of rows of the input image
        Returns
        -------
        x : torch tensor
            4D tensor output by the network.
                first dimension = batch size
                second dimension = number of channels
                third dimension = number of columns of the target
                fourth dimension = number of rows of the target
        """
        encoder_outs = []

        # If input height and/or width are odd, make it even (pad it).
        padIt = False
        pad3 = 0
        pad2 = 0

        if x.shape[3] % 2 != 0:
            padIt = True
            pad3 = 1
        if x.shape[2] % 2 != 0:
            padIt = True
            pad2 = 1
        if padIt:
            p2d = (0, pad3, 0, pad2)
            x = nn.functional.pad(x, p2d, 'constant', 0)

        # Encoder.
        for i, module in enumerate(self.down_convs):
            x, x_0 = module(x)
            encoder_outs.append(x_0)

        # Decoder.
        encoder_outs.reverse()
        for i, module in enumerate(self.up_convs):
            x_0 = encoder_outs[i+1]
            x = module(x_0, x)

        # If the input has been padded, crop the output.
        if padIt:
            x = x[:, :, 0:(x.shape[2]-pad2), 0:(x.shape[3]-pad3)]

        return (x,)
